{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import scipy\n",
    "import librosa\n",
    "import sed_eval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "\n",
    "import csv\n",
    "import dcase_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DatasetLoader(TrainPath,ValidatePath,TestPath):\n",
    "    #train    \n",
    "    train_data = np.load(TrainPath,allow_pickle=True)\n",
    "    #validation    \n",
    "    validate_data = np.load(ValidatePath,allow_pickle=True)\n",
    "    #test    \n",
    "    test_data = np.load(TestPath,allow_pickle=True)\n",
    "\n",
    "    #Processing all the loaded dataset\n",
    "    Train = {}\n",
    "    print(train_data.shape)\n",
    "    Train_Data = np.c_[train_data[:,0].tolist()]\n",
    "    Train_Label = np.c_[train_data[:,1].tolist()]\n",
    "    Train_Data = np.hstack(Train_Data).transpose() #sklearn requires data in [n_samples,n_features]\n",
    "    Train_Label = np.hstack(Train_Label)\n",
    "    Train['Data'] = Train_Data\n",
    "    Train['Label'] = Train_Label\n",
    "\n",
    "    Validation = {}\n",
    "    Validate_Data = np.c_[validate_data[:,0].tolist()]\n",
    "    Validate_Label = np.c_[validate_data[:,1].tolist()]\n",
    "    Validate_Data = np.hstack(Validate_Data).transpose() #sklearn requires data in [n_samples,n_features]\n",
    "    Validate_Label = np.hstack(Validate_Label)\n",
    "    Validation['Data'] = Validate_Data\n",
    "    Validation['Label'] = Validate_Label\n",
    "\n",
    "    Test = {}\n",
    "    Test_Data = np.c_[test_data[:,0].tolist()]\n",
    "    Test_Label = np.c_[test_data[:,1].tolist()]\n",
    "    Test_Data = np.hstack(Test_Data).transpose() #sklearn requires data in [n_samples,n_features]\n",
    "    Test_Label = np.hstack(Test_Label)\n",
    "    Test['Data'] = Test_Data\n",
    "    Test['Label'] = Test_Label\n",
    "    \n",
    "    return Train,Validation,Test\n",
    "\n",
    "def Evaluation(y_true,y_pred,y_pred_prob):\n",
    "    print('=============precision_recall_fscore_support======================')\n",
    "    print(metrics.precision_recall_fscore_support(y_true, y_pred, average=\"binary\"))\n",
    "    print('===================Report=======================')\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print('===================Accuracy=======================')\n",
    "    print(metrics.accuracy_score(y_true, y_pred,normalize=True,sample_weight=None))\n",
    "    print('===================ConfusionMatrix=======================')\n",
    "    c_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(1)\n",
    "    plot_confusion_matrix(c_matrix, classes=[0, 1], normalize = True, title='Confusion matrix with normalization')\n",
    "    plt.show()\n",
    "\n",
    "    print('===================ROC curve======================')\n",
    "    #Roc Auc\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true,y_pred)\n",
    "    plt.figure(2)\n",
    "    plt.plot(fpr, tpr, label='RF')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    print('===================AUC score=======================')\n",
    "    print(metrics.roc_auc_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Write_Estimated_Annotation(Predicted_Label_Vector,AnAnnotation,AllAnnotations,Parameters):\n",
    "    '''\n",
    "    Predicted_Label_Vector:The predicted label vector generated by classifier\n",
    "    AnAnnotation: a list for only storing a pair of onset&offset for single event\n",
    "    AllAnnotaions: the list for storing multiple AnAnnotation-lists for one processing file.\n",
    "    '''\n",
    "    for i in range(len(Predicted_Label_Vector)-1):\n",
    "                    #If next frame is different with current frame\n",
    "                    if Predicted_Label_Vector[i] != Predicted_Label_Vector[i+1]:\n",
    "                        #Next Frame:Speech; \n",
    "                        if Predicted_Label_Vector[i+1] - Predicted_Label_Vector[i] > 0:\n",
    "                            #Append this Onset\n",
    "                            AnAnnotation.append((i+1)*Parameters['hop_size']/Parameters['sampling_rate'])\n",
    "                        #Next Frame:Non speech\n",
    "                        elif Predicted_Label_Vector[i+1] - Predicted_Label_Vector[i] < 0:\n",
    "                            #Append this Offset\n",
    "                            AnAnnotation.append((i+1)*Parameters['hop_size']/Parameters['sampling_rate'])\n",
    "                            AnAnnotation.append('speech')\n",
    "                            if len(AnAnnotation) == 2:\n",
    "                                AnAnnotation.insert(0,0)\n",
    "                            AllAnnotations.append(AnAnnotation)\n",
    "                            AnAnnotation = []\n",
    "                    #Insert the ending frame and class label manually if the speech last til the end\n",
    "                    if len(Predicted_Label_Vector) - 2 == i and Predicted_Label_Vector[i] == 1:\n",
    "                        AnAnnotation.append(np.floor(len(Predicted_Label_Vector)*Parameters['hop_size']/Parameters['sampling_rate']))\n",
    "                        AnAnnotation.append('speech')\n",
    "                        if len(AnAnnotation) == 2:\n",
    "                            AnAnnotation.insert(0,0)\n",
    "                        AllAnnotations.append(AnAnnotation)\n",
    "                        AnAnnotation = []\n",
    "    return AllAnnotations\n",
    "\n",
    "def Annotations_Evaluation(File_List,Annotated_Data):\n",
    "    #File_List: the list of dictionaries that save all reference and estimation txt pair. see detail in doc of sed_eval\n",
    "    #Annotated_Data: empty list for saving all \n",
    "    \n",
    "    # Get used event labels\n",
    "    all_data = dcase_util.containers.MetaDataContainer()\n",
    "    #load both ref and est event lists\n",
    "    for file_pair in File_List:\n",
    "        reference_event_list = sed_eval.io.load_event_list(\n",
    "                filename=file_pair['reference_file']\n",
    "        )\n",
    "        ref_speech_list = sed_eval.util.event_list.filter_event_list(reference_event_list, scene_label=None, event_label='speech', filename=None)\n",
    "        \n",
    "        estimated_event_list = sed_eval.io.load_event_list(\n",
    "                filename=file_pair['estimated_file']\n",
    "        )\n",
    "        est_speech_list = sed_eval.util.event_list.filter_event_list(estimated_event_list, scene_label=None, event_label='speech', filename=None)\n",
    "\n",
    "        Annotated_Data.append({'ref_speech_list': ref_speech_list,\n",
    "                        'est_speech_list': est_speech_list})\n",
    "        all_data += ref_speech_list\n",
    "    # Start evaluating\n",
    "    # Create metrics classes, define parameters    \n",
    "    event_labels = all_data.unique_event_labels\n",
    "    segment_based_metrics = sed_eval.sound_event.SegmentBasedMetrics(\n",
    "        event_label_list=event_labels,\n",
    "        time_resolution=0.1\n",
    "        )\n",
    "    event_based_metrics = sed_eval.sound_event.EventBasedMetrics(\n",
    "    event_label_list=event_labels,\n",
    "    t_collar=0.250\n",
    "    )\n",
    "    # Go through files\n",
    "    for file_pair in Annotated_Data:\n",
    "        segment_based_metrics.evaluate(\n",
    "                reference_event_list=file_pair['ref_speech_list'],\n",
    "                estimated_event_list=file_pair['est_speech_list']\n",
    "        )\n",
    "\n",
    "        event_based_metrics.evaluate(\n",
    "                reference_event_list=file_pair['ref_speech_list'],\n",
    "                estimated_event_list=file_pair['est_speech_list']\n",
    "        )\n",
    "    # Get only certain metrics\n",
    "    overall_segment_based_metrics = segment_based_metrics.results_overall_metrics()\n",
    "    print('-------------------------------------')\n",
    "    print(\"Accuracy:\", overall_segment_based_metrics['accuracy']['accuracy'])\n",
    "    # print all metrics as reports\n",
    "    print(segment_based_metrics)\n",
    "    \n",
    "def Prediction_Evaluation(Dir,Estimator,Parameters,TransitionMatrix):\n",
    "    #Dir: the path of the fold that contains of the test data\n",
    "    #Estimator: Pre trained testimator\n",
    "    \n",
    "    file_list = []#list for storing  annotated_data over all files\n",
    "    annotated_data = [] #list for storing all annotation pairs\n",
    "\n",
    "    for root, dirs,files in os.walk(Dir):\n",
    "        for file in os.listdir(root):   \n",
    "            if file.endswith('.txt'):  \n",
    "                # a dictionary contained ref and est pair is required by sed_eval\n",
    "                ref_est_pair = {}\n",
    "                AnAnnotation = [] #a list of single onset&offset pair\n",
    "                AllAnnotations = []#a list for storing all onset&offset of speech events for one file\n",
    "                \n",
    "                #Get the processing filename without extension\n",
    "                file_name = os.path.splitext(file)[0]\n",
    "                \n",
    "                #corresponding .wav file path\n",
    "                audio_file_path = root + '/' + file_name + '.wav'                \n",
    "                #This is the txt path for storing estimated annotation\n",
    "                estimated_txt_path = os.getcwd() + '/estimated_txt/'  + file_name + '_estimated.txt'                \n",
    "                #corresponding reference txt path\n",
    "                reference_txt_path = root + file #the .txt file would be used as reference\n",
    "                \n",
    "                print(reference_txt_path)\n",
    "                ref_est_pair['reference_file'] = reference_txt_path\n",
    "                ref_est_pair['estimated_file'] = estimated_txt_path\n",
    "                \n",
    "                #Load Annotated Information and read speech event only                \n",
    "                annotated_event = sed_eval.io.load_event_list(reference_txt_path)\n",
    "                speech_event = sed_eval.util.event_list.filter_event_list(annotated_event, scene_label=None, event_label='speech', filename=None)               \n",
    "                \n",
    "                #process all audio data                     \n",
    "                feature_vector,audio_data, audio_sr = FeatureExtraction(audio_file_path,Parameters)\n",
    "                #Get Feature vector\n",
    "                feature_vector = feature_vector.transpose()\n",
    "                #Get true label vector\n",
    "                y_label = CreateLabelVector(Data=audio_data,\n",
    "                                                EventList=speech_event,\n",
    "                                                Parameters=Parameters,\n",
    "                                                LabelIndex=1) \n",
    "                #Predict the probability of speech for each frame\n",
    "                y_pred_speech_prob = Estimator.predict_proba(feature_vector)[:,1] #[0][0] is non-speech; [0][1] is speech\n",
    "                #viterbi smoothing\n",
    "                viterbi_sequence = librosa.sequence.viterbi_binary(y_pred_speech_prob, TransitionMatrix, p_state=0.5, p_init=None).ravel()\n",
    "                \n",
    "                #create estimated annotation txt file based on predicted label vector, write all onset&offset pairs of speech events in a list\n",
    "                estimated_speech_annotations = Write_Estimated_Annotation(Predicted_Label_Vector = viterbi_sequence,\n",
    "                                                                          AnAnnotation = AnAnnotation,\n",
    "                                                                          AllAnnotations = AllAnnotations,\n",
    "                                                                          Parameters = Parameters)               \n",
    "#=====================================================================   \n",
    "                #write the estimated annotations\n",
    "                with open(estimated_txt_path, 'w') as csv_file:\n",
    "                    writer = csv.writer(csv_file, delimiter='\\t')\n",
    "                    writer.writerows(estimated_speech_annotations)\n",
    "                file_list.append(ref_est_pair)\n",
    "#=============================use sed_eval to evaluate the prediction=========  \n",
    "    #file_list\n",
    "    #annotated_data\n",
    "    Annotations_Evaluation(File_List = file_list,\n",
    "                           Annotated_Data = annotated_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (40,431) into shape (40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   2879\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2880\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2881\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ndim'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-c3248f6657e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#Load all the data and label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mTrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mValidation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainSetPath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mValidateSetPath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTestSetPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mTrain_Data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrain_Label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mValidate_Data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mValidate_Label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTest_Data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTest_Label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mValidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mValidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-185-cc961f18554b>\u001b[0m in \u001b[0;36mDatasetLoader\u001b[0;34m(TrainPath, ValidatePath, TestPath)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#Processing all the loaded dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mTrain_Data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mTrain_Label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mTrain_Data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_Data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sklearn requires data in [n_samples,n_features]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0mscalartypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mitem_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0mnewobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrans1d\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mitem_ndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   2880\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2881\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2882\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (40,431) into shape (40)"
     ]
    }
   ],
   "source": [
    "#Load the dataset\n",
    "TrainSetPath = os.getcwd()+'/JPNotebookExported/Train_Dataset.npy'\n",
    "ValidateSetPath = os.getcwd()+'/JPNotebookExported/Validate_Dataset.npy'\n",
    "TestSetPath = os.getcwd()+'/JPNotebookExported/Test_Dataset.npy'\n",
    "\n",
    "#Load Transition Matrix\n",
    "train_transition_matrix = np.load(os.getcwd()+'/JPNotebookExported/Train_TransitionMatrix.npy',allow_pickle=True)\n",
    "validate_transition_matrix = np.load(os.getcwd()+'/JPNotebookExported/Validate_TransitionMatrix.npy',allow_pickle=True)\n",
    "test_transition_matrix = np.load(os.getcwd()+'/JPNotebookExported/Test_TransitionMatrix.npy',allow_pickle=True)\n",
    "\n",
    "#Load all the data and label\n",
    "Train,Validation,Test = DatasetLoader(TrainSetPath,ValidateSetPath,TestSetPath)\n",
    "Train_Data,Train_Label,Validate_Data,Validate_Label,Test_Data,Test_Label = Train['Data'], Train['Label'],Validation['Data'],Validation['Label'],Test['Data'],Test['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make own scoring functions\n",
    "\n",
    "def report_wrapper(y_true, y_pred): \n",
    "    #wrapper of metrics.classification_report\n",
    "    return metrics.classification_report(y_true,y_pred)\n",
    "\n",
    "def confusion_matrix_wrapper(y_true, y_pred): \n",
    "    return metrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "def recall_score_wrapper(y_true, y_pred): \n",
    "    return metrics.recall_score(y_true, y_pred)\n",
    "\n",
    "def roc_auc_wrapper(y_true, y_pred): \n",
    "    return metrics.roc_auc_score(y_true, y_pred)\n",
    "\n",
    "def f1_score_wrapper(y_true, y_pred): \n",
    "    return metrics.f1_score(y_true, y_pred)\n",
    "\n",
    "def accuracy_wrapper(y_true, y_pred): \n",
    "    return metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "def precision_wrapper(y_true, y_pred): \n",
    "    return metrics.precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Train using best parameters\\nbest_forest = grid_search.best_estimator_\\n# Predictions\\n#pred_train = best_forest.predict(train_data)\\npred_validate = best_forest.predict(Validate_Data)\\n# Accuracy\\n#train_acc = accuracy_score(train_label, pred_train)\\ntest_acc = accuracy_score(Validate_Label, pred_validate)\\n#print (\"train acc: {0:.2f}, test acc: {1:.2f}\".format(train_acc, test_acc))\\n# Calculate other evaluation metrics \\n#precision, recall, F1, _ = precision_recall_fscore_support(Validate_Label, pred_validate, average=\"binary\")\\n#print (\"precision: {0:.2f}. recall: {1:.2f}, F1: {2:.2f}\".format(precision, recall, F1))\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Params = {\n",
    "        'sampling_rate':22050,\n",
    "        'win_size': 1024,\n",
    "        'hop_size': 512,\n",
    "        'min_freq': 80,\n",
    "        'max_freq': 8000,\n",
    "        'num_mel_filters': 128,\n",
    "        'n_dct': 20}\n",
    "#Grid search with cross validation\n",
    "\n",
    "#Create the parameters grid\n",
    "params_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [5,15],\n",
    "    'max_features': [40],\n",
    "    'min_samples_leaf': [1,5],\n",
    "    'min_samples_split': [4,8],\n",
    "    'n_estimators': [5,15,35]\n",
    "}\n",
    "\n",
    "#Define your scoring strategy\n",
    "#we can list a bunch of scoring functions here that predined by ssklearn\n",
    "#Think about your problem first, then pick your effective ones\n",
    "#scoring = ['recall','roc_auc']\n",
    "scoring = {#'report_wrapper': make_scorer(report_wrapper),\n",
    "           #'confusion_matrix_wrapper': make_scorer(confusion_matrix_wrapper),\n",
    "           'recall_score_wrapper': make_scorer(recall_score_wrapper),\n",
    "           'roc_auc_wrapper': make_scorer(roc_auc_wrapper),\n",
    "           'f1_score_wrapper': make_scorer(f1_score_wrapper),\n",
    "           'accuracy_wrapper': make_scorer(accuracy_wrapper),\n",
    "           'precision_wrapper': make_scorer(precision_wrapper)}\n",
    "\n",
    "# Instantiate a RF classfier\n",
    "RFclf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=5, min_samples_split=4,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=35, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "# Instantiate grid search\n",
    "RF_grid_search = GridSearchCV(estimator=RFclf, param_grid=params_grid, cv=5, \n",
    "                           n_jobs=-1, verbose=0, scoring=scoring,refit = 'recall_score_wrapper')\n",
    "# Fit grid search to the data\n",
    "RF_grid_search.fit(Train_Data, Train_Label)\n",
    "\n",
    "# Train using best parameters\n",
    "bestRF = RF_grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "pred_Train = bestRF.predict(Train_Data)\n",
    "pred_Validate = bestRF.predict(Validate_Data)\n",
    "pred_Test = bestRF.predict(Test_Data)\n",
    "\n",
    "# Predicted probability\n",
    "pred_Train_prob = bestRF.predict_proba(Train_Data)[:,1]\n",
    "pred_Validate_prob = bestRF.predict_proba(Validate_Data)[:,1]\n",
    "pred_Test_prob = bestRF.predict_proba(Test_Data)[:,1]\n",
    "\n",
    "# Evaluation\n",
    "Validate_Data_path =  os.getcwd() + '/1_Dataset_Generate/audio/soundbanks/Validate/generated/bimodal/'\n",
    "Prediction_Evaluation(Validate_Data_path,bestRF,Params,train_transition_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=55, max_features=40, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=5,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestRF_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
