{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import muda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(Files_Dir,Parameters,Property,EventLabel='speech'):\n",
    "    #Process all the .wav, .txt, in the assigned folder\n",
    "\n",
    "    #Files_Dir: Target folder for saving all raw audio data\n",
    "    #Parameters: Using  for processing audio files\n",
    "    #Property:Create train/test/validation set\n",
    "    #EventLabel: The label in string which indicate the class you want to learn,see sed_eval doc to see detail\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    #Calculate the Transition Matrix From Training set,saving the probability of each status transition\n",
    "    SS = []#Speech to Speech\n",
    "    NN = []#NonSpeech to NonSpeech\n",
    "    SN = []#Speech to NonSpeech\n",
    "    NS = []#NonSpeech to Speech\n",
    "\n",
    "    file_index = 0\n",
    "    for root, dirs,files in os.walk(Files_Dir):\n",
    "        #.txt-based: which means processing txt first then find its corresponding .wav file\n",
    "\n",
    "        #root_path,subfolders = root,dirs\n",
    "        for file in os.listdir(root):\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = root + '/' + file\n",
    "                \n",
    "                processed_filename =  os.path.splitext(file)[0]\n",
    "                #Load Annotated Information\n",
    "                annotated_event = sed_eval.io.load_event_list(file_path)\n",
    "                target_event = sed_eval.util.event_list.filter_event_list(annotated_event, scene_label=None, event_label=EventLabel, filename=None)\n",
    "\n",
    "                #Load Audio for feature extraction\n",
    "                audio_file_name = os.path.splitext(file)[0] + '.wav'\n",
    "                audio_file_path = root + '/' + audio_file_name\n",
    "                feature_vector,audio_data, audio_sr = FeatureExtraction(audio_file_path,Parameters)\n",
    "\n",
    "                #Using Annotation info to create the vector labels\n",
    "                label_vector = CreateLabelVector(Data=audio_data,\n",
    "                                                     EventList=target_event,\n",
    "                                                     Parameters=Parameters,\n",
    "                                                     LabelIndex=1)\n",
    "                if feature_vector.shape[1] != len(label_vector):\n",
    "                    print('===========Waring! Unmatched data size,will skip this file:==========')\n",
    "                    print(file_path)\n",
    "                    continue\n",
    "                #Calculate the probability in the transition state matrix If data contains speech\n",
    "                if sum(label_vector) != 0:\n",
    "                    #probability of ss,nn,sn,ns; n=nonspeech, s = speech\n",
    "                    p_ss, p_nn, p_sn, p_ns = ComputeStateTransition(label_vector)\n",
    "                    SS.append(p_ss)\n",
    "                    NN.append(p_nn)\n",
    "                    SN.append(p_sn)\n",
    "                    NS.append(p_ns)\n",
    "\n",
    "                data.append([feature_vector,label_vector])\n",
    "                file_index += 1\n",
    "\n",
    "    #Transition Matrix:\n",
    "    trans_matrix = np.array([[np.mean(SS),1 - np.mean(SS)],[1 - np.mean(NN),np.mean(NN)]])\n",
    "    #Saving the transition Matrix\n",
    "    np.save(os.getcwd()+'/JPNotebookExported/' + Property + '_TransitionMatrix.npy', trans_matrix)\n",
    "    np.save(os.getcwd()+'/JPNotebookExported/' + Property + '_Dataset.npy', np.asarray(data))\n",
    "    print(trans_matrix)\n",
    "\n",
    "    return np.asarray(data),trans_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DeformDataset(DatasetPath,OutputPath,DeformerBook):\n",
    "    for item in DeformerBook:\n",
    "        #Create directory for saving deformed dataset(original dataset excluded)\n",
    "        dirName = OutputPath + '/' + item\n",
    "        os.mkdir(dirName)\n",
    "        deformer = DeformerBook[item]\n",
    "        for root, dirs,files in os.walk(DatasetPath):\n",
    "            for file in os.listdir(root):  \n",
    "                if file.endswith(\".jams\"):\n",
    "                    file_name = os.path.splitext(file)[0]\n",
    "\n",
    "                    jams_path = root + '/' + file_name + '.jams'\n",
    "                    audio_path = root + '/' + file_name + '.wav'\n",
    "\n",
    "                    output_path = dirName + '/' + file_name\n",
    "                    # Load an example audio file with annotation\n",
    "                    j_orig = muda.load_jam_audio(jams_path, audio_path)            \n",
    "\n",
    "                    #for j_new in pipeline.transform(j_orig):\n",
    "                        #print(j_new)\n",
    "                    for i, jam_out in enumerate(deformer.transform(j_orig)):\n",
    "                        muda.save(output_path + '_deformed_{:02d}.wav'.format(i),\n",
    "                                   output_path + '_deformed_{:02d}.jams'.format(i),\n",
    "                                   jam_out)\n",
    "\n",
    "                        #load scaper annotations\n",
    "                        ann = jam_out.annotations.search(namespace='scaper')[0]\n",
    "                        #Write new txt file\n",
    "                        txt_path = output_path + '_deformed_{:02d}.txt'.format(i)\n",
    "                        csv_data = []\n",
    "                        for obs in ann.data:\n",
    "                            if obs.value['role'] == 'foreground':\n",
    "                                csv_data.append(\n",
    "                                    [obs.time, obs.time+obs.duration, obs.value['label']])\n",
    "\n",
    "                        with open(txt_path, 'w') as csv_file:\n",
    "                            writer = csv.writer(csv_file, delimiter='\\t')\n",
    "                            writer.writerows(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize deformer object\n",
    "pitch_shift_1 = muda.deformers.PitchShift(n_semitones=-1)\n",
    "pitch_shift_2 = muda.deformers.PitchShift(n_semitones=1)\n",
    "time_stretch = muda.deformers.RandomTimeStretch(n_samples=2,scale=0.3)\n",
    "colored_noise = muda.deformers.ColoredNoise(n_samples=1,color=['white'])\n",
    "drc = muda.deformers.DynamicRangeCompression(preset='speech')\n",
    "#ir_convolution = muda.deformers.IRConvolution()\n",
    "\n",
    "#setup the directory of dataset\n",
    "dataset_dir = os.getcwd() + '/AugmentedDataset/soundbanks'\n",
    "output_dir = os.getcwd() + '/AugmentedDataset/augmentedsoundbanks'\n",
    "\n",
    "deformers = {#'PitchShift1':pitch_shift_1,\n",
    "             #'PitchShift2':pitch_shift_2,\n",
    "             #'TimeStretched':time_stretch,\n",
    "             #'ColoredNoise':colored_noise,\n",
    "             'DynamicCompressed':drc}\n",
    "\n",
    "DeformDataset(DatasetPath = dataset_dir,\n",
    "             OutputPath = output_dir,\n",
    "             DeformerBook = deformers)\n",
    "\n",
    "#Create the deformed dataset\n",
    "DeformedTrainData,DeformedTrainTransMatrix = CreateDataset(Files_Dir = output_dir,\n",
    "                                            Parameters = Params,\n",
    "                                            Property = 'DeformedTrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [5, 11, 18, 25, 32, 38, 45, 52, 59, 66, 72, 79, 86, 93, 100], 'max_features': ['auto', None], 'max_depth': [2, 9, 17, 25, 33, 41, 48, 56, 64, 72, 80, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
